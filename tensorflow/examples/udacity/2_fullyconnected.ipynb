{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 2\n",
    "------------\n",
    "\n",
    "Previously in `1_notmnist.ipynb`, we created a pickle with formatted datasets for training, development and testing on the [notMNIST dataset](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html).\n",
    "\n",
    "The goal of this assignment is to progressively train deeper and more accurate models using TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19456,
     "status": "ok",
     "timestamp": 1449847956073,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "0ddb1607-1fc4-4ddb-de28-6c7ab7fb0c33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 19723,
     "status": "ok",
     "timestamp": 1449847956364,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "2ba0fc75-1487-4ace-a562-cf81cae82793"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCLVqyQ5vPPH"
   },
   "source": [
    "We're first going to train a multinomial logistic regression using simple gradient descent.\n",
    "\n",
    "TensorFlow works like this:\n",
    "* First you describe the computation that you want to see performed: what the inputs, the variables, and the operations look like. These get created as nodes over a computation graph. This description is all contained within the block below:\n",
    "\n",
    "      with graph.as_default():\n",
    "          ...\n",
    "\n",
    "* Then you can run the operations on this graph as many times as you want by calling `session.run()`, providing it outputs to fetch from the graph that get returned. This runtime operation is all contained in the block below:\n",
    "\n",
    "      with tf.Session(graph=graph) as session:\n",
    "          ...\n",
    "\n",
    "Let's load all the data into TensorFlow and build the computation graph corresponding to our training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Nfv39qvtvOl_"
   },
   "outputs": [],
   "source": [
    "# With gradient descent training, even this much data is prohibitive.\n",
    "# Subset the training data for faster turnaround.\n",
    "train_subset = 10000 # Number of training examples\n",
    "\n",
    "graph = tf.Graph() # Instantiate a new tensorflow graph\n",
    "with graph.as_default(): # Attach the graph\n",
    "\n",
    "    # Input data.\n",
    "    # Load the training, validation and test data into constants that are\n",
    "    # attached to the graph.\n",
    "    tf_train_dataset = tf.constant(train_dataset[:train_subset, :]) # tf.constant() makes the datasets visible to the graph\n",
    "    tf_train_labels = tf.constant(train_labels[:train_subset])\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    # These are the parameters that we are going to be training. The weight\n",
    "    # matrix will be initialized using random values following a (truncated)\n",
    "    # normal distribution. The biases get initialized to zero.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels])) # Parameters are initialized using tf.Variable()\n",
    "    biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    # We multiply the inputs with the weight matrix, and add biases. We compute\n",
    "    # the softmax and cross-entropy (it's one operation in TensorFlow, because\n",
    "    # it's very common, and it can be optimized). We take the average of this\n",
    "    # cross-entropy across all training examples: that's our loss.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases # XW + b\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    # ^ Compute the average cross_entropy\n",
    "    # tf.nn.softmax_cross_entropy_with_logits() returns the cross-entropy computed on the results of softmax(logits)\n",
    "  \n",
    "    # Optimizer.\n",
    "    # We are going to find the minimum of this loss using gradient descent.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # Optimizer object (0.5 = learning rate)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    # These are not part of training, but merely here so that we can report\n",
    "    # accuracy figures as we train.\n",
    "    train_prediction = tf.nn.softmax(logits) # Class probabilities (during training)\n",
    "    valid_prediction = tf.nn.softmax(        # Class probabilities (for cross-validation)\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases) # Class probabilities (for testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process we went through when setting up the graph was:\n",
    "\n",
    "0. Instantiated the graph, then attached it. Note that we specified the number of training examples here.\n",
    "1. Introduce the datasets to the graph via tf.constant()\n",
    "2. Introduce the model's parameters - W and b - to the graph via tf.Variable() (we also initialized these)\n",
    "3. Expressed the loss function - this took the softmax probabilities as inputs.\n",
    "4. Created an optimization object and associated the loss function with it.\n",
    "5. Defined feedback parameters to display during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KQcL4uqISHjP"
   },
   "source": [
    "Let's run this computation and iterate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 9
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 57454,
     "status": "ok",
     "timestamp": 1449847994134,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "z2cjdenH869W",
    "outputId": "4c037ba1-b526-4d8e-e632-91e2a0333267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "Loss at step 0: 20.384365\n",
      "Training accuracy: 6.9%\n",
      "Validation accuracy: 9.3%\n",
      "Test accuracy: 9.3%\n",
      "Loss at step 100: 2.291604\n",
      "Training accuracy: 71.2%\n",
      "Validation accuracy: 70.8%\n",
      "Test accuracy: 78.2%\n",
      "Loss at step 200: 1.856400\n",
      "Training accuracy: 73.9%\n",
      "Validation accuracy: 73.1%\n",
      "Test accuracy: 80.2%\n",
      "Loss at step 300: 1.616425\n",
      "Training accuracy: 75.3%\n",
      "Validation accuracy: 74.0%\n",
      "Test accuracy: 81.0%\n",
      "Loss at step 400: 1.452093\n",
      "Training accuracy: 76.0%\n",
      "Validation accuracy: 74.6%\n",
      "Test accuracy: 81.6%\n",
      "Loss at step 500: 1.329108\n",
      "Training accuracy: 77.0%\n",
      "Validation accuracy: 75.1%\n",
      "Test accuracy: 82.0%\n",
      "Loss at step 600: 1.232408\n",
      "Training accuracy: 77.5%\n",
      "Validation accuracy: 75.3%\n",
      "Test accuracy: 82.2%\n",
      "Loss at step 700: 1.153724\n",
      "Training accuracy: 77.9%\n",
      "Validation accuracy: 75.5%\n",
      "Test accuracy: 82.5%\n",
      "Loss at step 800: 1.088146\n",
      "Training accuracy: 78.5%\n",
      "Validation accuracy: 75.7%\n",
      "Test accuracy: 82.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 801\n",
    "\n",
    "def accuracy(predictions, labels): # Computes prediction accuracy as a percentage\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "        / predictions.shape[0])\n",
    "\n",
    "with tf.Session(graph=graph) as session: # Intantiate a TensorFlow session, specifying the graph we made as an input.\n",
    "    # This is a one-time operation which ensures the parameters get initialized as\n",
    "    # we described in the graph: random weights for the matrix, zeros for the\n",
    "    # biases. \n",
    "    tf.global_variables_initializer().run() # Initialize random variables\n",
    "    print('Initialized!')\n",
    "    for step in range(num_steps): # Run the number of iterations we say to\n",
    "        # Run the computations. We tell .run() that we want to run the optimizer,\n",
    "        # and get the loss value and the training predictions returned as numpy\n",
    "        # arrays.\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction]) # .run() steps through one iteration of optimization\n",
    "        if (step % 100 == 0): # Report the loss function's value and classifier accuracy every 100 steps\n",
    "            print('Loss at step %d: %f' % (step, l))\n",
    "            print('Training accuracy: %.1f%%' % accuracy(predictions, train_labels[:train_subset, :]))\n",
    "            # Calling .eval() on valid_prediction is basically like calling run(), but\n",
    "            # just to get that one numpy array. Note that it recomputes all its graph\n",
    "            # dependencies. (.eval() computes a particular variable's value)\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "            print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after defining our graph, we passsed it to a Session, in which we:\n",
    "1. Initialized the constants with random values.\n",
    "2. Stepped through optimization a fixed number of times by calling .run([optimizer, loss, training_predictions]) within each iteration of a for loop.\n",
    "3. Printed our performance by computing baked-in expressions via .eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x68f-hxRGm3H"
   },
   "source": [
    "Let's now switch to stochastic gradient descent training instead, which is much faster.\n",
    "\n",
    "The graph will be similar, except that instead of holding all the training data into a constant node, we create a `Placeholder` node which will be fed actual data at every call of `session.run()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "qhPMzWYRGrzM"
   },
   "outputs": [],
   "source": [
    "batch_size = 128 # Specify the number of examples to use at each iteration's loss function computation\n",
    "\n",
    "graph = tf.Graph() # Instantiate the graph\n",
    "with graph.as_default(): # Attach the graph\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size)) # We use tf.placeholder() to allow data to be passed at runtime\n",
    "    tf_train_labels  = tf.placeholder(tf.float32, shape=(batch_size, num_labels)) # num_labels is the number of classes\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset  = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    weights = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, num_labels])) # Same as before\n",
    "    biases  = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = tf.matmul(tf_train_dataset, weights) + biases # As before\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    # As before, we compute the mean cross-entropy\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss) # We use a fixed learning rate\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XmVZESmtG4JH"
   },
   "source": [
    "Let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 6
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 66292,
     "status": "ok",
     "timestamp": 1449848003013,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "FoF91pknG_YW",
    "outputId": "d255c80e-954d-4183-ca1c-c7333ce91d0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "0\n",
      "Minibatch loss at step 0: 16.466751\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 15.6%\n",
      "128\n",
      "256\n",
      "384\n",
      "512\n",
      "640\n",
      "768\n",
      "896\n",
      "1024\n",
      "1152\n",
      "1280\n",
      "1408\n",
      "1536\n",
      "1664\n",
      "1792\n",
      "1920\n",
      "2048\n",
      "2176\n",
      "2304\n",
      "2432\n",
      "2560\n",
      "2688\n",
      "2816\n",
      "2944\n",
      "3072\n",
      "3200\n",
      "3328\n",
      "3456\n",
      "3584\n",
      "3712\n",
      "3840\n",
      "3968\n",
      "4096\n",
      "4224\n",
      "4352\n",
      "4480\n",
      "4608\n",
      "4736\n",
      "4864\n",
      "4992\n",
      "5120\n",
      "5248\n",
      "5376\n",
      "5504\n",
      "5632\n",
      "5760\n",
      "5888\n",
      "6016\n",
      "6144\n",
      "6272\n",
      "6400\n",
      "6528\n",
      "6656\n",
      "6784\n",
      "6912\n",
      "7040\n",
      "7168\n",
      "7296\n",
      "7424\n",
      "7552\n",
      "7680\n",
      "7808\n",
      "7936\n",
      "8064\n",
      "8192\n",
      "8320\n",
      "8448\n",
      "8576\n",
      "8704\n",
      "8832\n",
      "8960\n",
      "9088\n",
      "9216\n",
      "9344\n",
      "9472\n",
      "9600\n",
      "9728\n",
      "9856\n",
      "9984\n",
      "10112\n",
      "10240\n",
      "10368\n",
      "10496\n",
      "10624\n",
      "10752\n",
      "10880\n",
      "11008\n",
      "11136\n",
      "11264\n",
      "11392\n",
      "11520\n",
      "11648\n",
      "11776\n",
      "11904\n",
      "12032\n",
      "12160\n",
      "12288\n",
      "12416\n",
      "12544\n",
      "12672\n",
      "12800\n",
      "12928\n",
      "13056\n",
      "13184\n",
      "13312\n",
      "13440\n",
      "13568\n",
      "13696\n",
      "13824\n",
      "13952\n",
      "14080\n",
      "14208\n",
      "14336\n",
      "14464\n",
      "14592\n",
      "14720\n",
      "14848\n",
      "14976\n",
      "15104\n",
      "15232\n",
      "15360\n",
      "15488\n",
      "15616\n",
      "15744\n",
      "15872\n",
      "16000\n",
      "16128\n",
      "16256\n",
      "16384\n",
      "16512\n",
      "16640\n",
      "16768\n",
      "16896\n",
      "17024\n",
      "17152\n",
      "17280\n",
      "17408\n",
      "17536\n",
      "17664\n",
      "17792\n",
      "17920\n",
      "18048\n",
      "18176\n",
      "18304\n",
      "18432\n",
      "18560\n",
      "18688\n",
      "18816\n",
      "18944\n",
      "19072\n",
      "19200\n",
      "19328\n",
      "19456\n",
      "19584\n",
      "19712\n",
      "19840\n",
      "19968\n",
      "20096\n",
      "20224\n",
      "20352\n",
      "20480\n",
      "20608\n",
      "20736\n",
      "20864\n",
      "20992\n",
      "21120\n",
      "21248\n",
      "21376\n",
      "21504\n",
      "21632\n",
      "21760\n",
      "21888\n",
      "22016\n",
      "22144\n",
      "22272\n",
      "22400\n",
      "22528\n",
      "22656\n",
      "22784\n",
      "22912\n",
      "23040\n",
      "23168\n",
      "23296\n",
      "23424\n",
      "23552\n",
      "23680\n",
      "23808\n",
      "23936\n",
      "24064\n",
      "24192\n",
      "24320\n",
      "24448\n",
      "24576\n",
      "24704\n",
      "24832\n",
      "24960\n",
      "25088\n",
      "25216\n",
      "25344\n",
      "25472\n",
      "25600\n",
      "25728\n",
      "25856\n",
      "25984\n",
      "26112\n",
      "26240\n",
      "26368\n",
      "26496\n",
      "26624\n",
      "26752\n",
      "26880\n",
      "27008\n",
      "27136\n",
      "27264\n",
      "27392\n",
      "27520\n",
      "27648\n",
      "27776\n",
      "27904\n",
      "28032\n",
      "28160\n",
      "28288\n",
      "28416\n",
      "28544\n",
      "28672\n",
      "28800\n",
      "28928\n",
      "29056\n",
      "29184\n",
      "29312\n",
      "29440\n",
      "29568\n",
      "29696\n",
      "29824\n",
      "29952\n",
      "30080\n",
      "30208\n",
      "30336\n",
      "30464\n",
      "30592\n",
      "30720\n",
      "30848\n",
      "30976\n",
      "31104\n",
      "31232\n",
      "31360\n",
      "31488\n",
      "31616\n",
      "31744\n",
      "31872\n",
      "32000\n",
      "32128\n",
      "32256\n",
      "32384\n",
      "32512\n",
      "32640\n",
      "32768\n",
      "32896\n",
      "33024\n",
      "33152\n",
      "33280\n",
      "33408\n",
      "33536\n",
      "33664\n",
      "33792\n",
      "33920\n",
      "34048\n",
      "34176\n",
      "34304\n",
      "34432\n",
      "34560\n",
      "34688\n",
      "34816\n",
      "34944\n",
      "35072\n",
      "35200\n",
      "35328\n",
      "35456\n",
      "35584\n",
      "35712\n",
      "35840\n",
      "35968\n",
      "36096\n",
      "36224\n",
      "36352\n",
      "36480\n",
      "36608\n",
      "36736\n",
      "36864\n",
      "36992\n",
      "37120\n",
      "37248\n",
      "37376\n",
      "37504\n",
      "37632\n",
      "37760\n",
      "37888\n",
      "38016\n",
      "38144\n",
      "38272\n",
      "38400\n",
      "38528\n",
      "38656\n",
      "38784\n",
      "38912\n",
      "39040\n",
      "39168\n",
      "39296\n",
      "39424\n",
      "39552\n",
      "39680\n",
      "39808\n",
      "39936\n",
      "40064\n",
      "40192\n",
      "40320\n",
      "40448\n",
      "40576\n",
      "40704\n",
      "40832\n",
      "40960\n",
      "41088\n",
      "41216\n",
      "41344\n",
      "41472\n",
      "41600\n",
      "41728\n",
      "41856\n",
      "41984\n",
      "42112\n",
      "42240\n",
      "42368\n",
      "42496\n",
      "42624\n",
      "42752\n",
      "42880\n",
      "43008\n",
      "43136\n",
      "43264\n",
      "43392\n",
      "43520\n",
      "43648\n",
      "43776\n",
      "43904\n",
      "44032\n",
      "44160\n",
      "44288\n",
      "44416\n",
      "44544\n",
      "44672\n",
      "44800\n",
      "44928\n",
      "45056\n",
      "45184\n",
      "45312\n",
      "45440\n",
      "45568\n",
      "45696\n",
      "45824\n",
      "45952\n",
      "46080\n",
      "46208\n",
      "46336\n",
      "46464\n",
      "46592\n",
      "46720\n",
      "46848\n",
      "46976\n",
      "47104\n",
      "47232\n",
      "47360\n",
      "47488\n",
      "47616\n",
      "47744\n",
      "47872\n",
      "48000\n",
      "48128\n",
      "48256\n",
      "48384\n",
      "48512\n",
      "48640\n",
      "48768\n",
      "48896\n",
      "49024\n",
      "49152\n",
      "49280\n",
      "49408\n",
      "49536\n",
      "49664\n",
      "49792\n",
      "49920\n",
      "50048\n",
      "50176\n",
      "50304\n",
      "50432\n",
      "50560\n",
      "50688\n",
      "50816\n",
      "50944\n",
      "51072\n",
      "51200\n",
      "51328\n",
      "51456\n",
      "51584\n",
      "51712\n",
      "51840\n",
      "51968\n",
      "52096\n",
      "52224\n",
      "52352\n",
      "52480\n",
      "52608\n",
      "52736\n",
      "52864\n",
      "52992\n",
      "53120\n",
      "53248\n",
      "53376\n",
      "53504\n",
      "53632\n",
      "53760\n",
      "53888\n",
      "54016\n",
      "54144\n",
      "54272\n",
      "54400\n",
      "54528\n",
      "54656\n",
      "54784\n",
      "54912\n",
      "55040\n",
      "55168\n",
      "55296\n",
      "55424\n",
      "55552\n",
      "55680\n",
      "55808\n",
      "55936\n",
      "56064\n",
      "56192\n",
      "56320\n",
      "56448\n",
      "56576\n",
      "56704\n",
      "56832\n",
      "56960\n",
      "57088\n",
      "57216\n",
      "57344\n",
      "57472\n",
      "57600\n",
      "57728\n",
      "57856\n",
      "57984\n",
      "58112\n",
      "58240\n",
      "58368\n",
      "58496\n",
      "58624\n",
      "58752\n",
      "58880\n",
      "59008\n",
      "59136\n",
      "59264\n",
      "59392\n",
      "59520\n",
      "59648\n",
      "59776\n",
      "59904\n",
      "60032\n",
      "60160\n",
      "60288\n",
      "60416\n",
      "60544\n",
      "60672\n",
      "60800\n",
      "60928\n",
      "61056\n",
      "61184\n",
      "61312\n",
      "61440\n",
      "61568\n",
      "61696\n",
      "61824\n",
      "61952\n",
      "62080\n",
      "62208\n",
      "62336\n",
      "62464\n",
      "62592\n",
      "62720\n",
      "62848\n",
      "62976\n",
      "63104\n",
      "63232\n",
      "63360\n",
      "63488\n",
      "63616\n",
      "63744\n",
      "63872\n",
      "64000\n",
      "Minibatch loss at step 500: 1.373646\n",
      "Minibatch accuracy: 78.1%\n",
      "Validation accuracy: 76.0%\n",
      "64128\n",
      "64256\n",
      "64384\n",
      "64512\n",
      "64640\n",
      "64768\n",
      "64896\n",
      "65024\n",
      "65152\n",
      "65280\n",
      "65408\n",
      "65536\n",
      "65664\n",
      "65792\n",
      "65920\n",
      "66048\n",
      "66176\n",
      "66304\n",
      "66432\n",
      "66560\n",
      "66688\n",
      "66816\n",
      "66944\n",
      "67072\n",
      "67200\n",
      "67328\n",
      "67456\n",
      "67584\n",
      "67712\n",
      "67840\n",
      "67968\n",
      "68096\n",
      "68224\n",
      "68352\n",
      "68480\n",
      "68608\n",
      "68736\n",
      "68864\n",
      "68992\n",
      "69120\n",
      "69248\n",
      "69376\n",
      "69504\n",
      "69632\n",
      "69760\n",
      "69888\n",
      "70016\n",
      "70144\n",
      "70272\n",
      "70400\n",
      "70528\n",
      "70656\n",
      "70784\n",
      "70912\n",
      "71040\n",
      "71168\n",
      "71296\n",
      "71424\n",
      "71552\n",
      "71680\n",
      "71808\n",
      "71936\n",
      "72064\n",
      "72192\n",
      "72320\n",
      "72448\n",
      "72576\n",
      "72704\n",
      "72832\n",
      "72960\n",
      "73088\n",
      "73216\n",
      "73344\n",
      "73472\n",
      "73600\n",
      "73728\n",
      "73856\n",
      "73984\n",
      "74112\n",
      "74240\n",
      "74368\n",
      "74496\n",
      "74624\n",
      "74752\n",
      "74880\n",
      "75008\n",
      "75136\n",
      "75264\n",
      "75392\n",
      "75520\n",
      "75648\n",
      "75776\n",
      "75904\n",
      "76032\n",
      "76160\n",
      "76288\n",
      "76416\n",
      "76544\n",
      "76672\n",
      "76800\n",
      "76928\n",
      "77056\n",
      "77184\n",
      "77312\n",
      "77440\n",
      "77568\n",
      "77696\n",
      "77824\n",
      "77952\n",
      "78080\n",
      "78208\n",
      "78336\n",
      "78464\n",
      "78592\n",
      "78720\n",
      "78848\n",
      "78976\n",
      "79104\n",
      "79232\n",
      "79360\n",
      "79488\n",
      "79616\n",
      "79744\n",
      "79872\n",
      "80000\n",
      "80128\n",
      "80256\n",
      "80384\n",
      "80512\n",
      "80640\n",
      "80768\n",
      "80896\n",
      "81024\n",
      "81152\n",
      "81280\n",
      "81408\n",
      "81536\n",
      "81664\n",
      "81792\n",
      "81920\n",
      "82048\n",
      "82176\n",
      "82304\n",
      "82432\n",
      "82560\n",
      "82688\n",
      "82816\n",
      "82944\n",
      "83072\n",
      "83200\n",
      "83328\n",
      "83456\n",
      "83584\n",
      "83712\n",
      "83840\n",
      "83968\n",
      "84096\n",
      "84224\n",
      "84352\n",
      "84480\n",
      "84608\n",
      "84736\n",
      "84864\n",
      "84992\n",
      "85120\n",
      "85248\n",
      "85376\n",
      "85504\n",
      "85632\n",
      "85760\n",
      "85888\n",
      "86016\n",
      "86144\n",
      "86272\n",
      "86400\n",
      "86528\n",
      "86656\n",
      "86784\n",
      "86912\n",
      "87040\n",
      "87168\n",
      "87296\n",
      "87424\n",
      "87552\n",
      "87680\n",
      "87808\n",
      "87936\n",
      "88064\n",
      "88192\n",
      "88320\n",
      "88448\n",
      "88576\n",
      "88704\n",
      "88832\n",
      "88960\n",
      "89088\n",
      "89216\n",
      "89344\n",
      "89472\n",
      "89600\n",
      "89728\n",
      "89856\n",
      "89984\n",
      "90112\n",
      "90240\n",
      "90368\n",
      "90496\n",
      "90624\n",
      "90752\n",
      "90880\n",
      "91008\n",
      "91136\n",
      "91264\n",
      "91392\n",
      "91520\n",
      "91648\n",
      "91776\n",
      "91904\n",
      "92032\n",
      "92160\n",
      "92288\n",
      "92416\n",
      "92544\n",
      "92672\n",
      "92800\n",
      "92928\n",
      "93056\n",
      "93184\n",
      "93312\n",
      "93440\n",
      "93568\n",
      "93696\n",
      "93824\n",
      "93952\n",
      "94080\n",
      "94208\n",
      "94336\n",
      "94464\n",
      "94592\n",
      "94720\n",
      "94848\n",
      "94976\n",
      "95104\n",
      "95232\n",
      "95360\n",
      "95488\n",
      "95616\n",
      "95744\n",
      "95872\n",
      "96000\n",
      "96128\n",
      "96256\n",
      "96384\n",
      "96512\n",
      "96640\n",
      "96768\n",
      "96896\n",
      "97024\n",
      "97152\n",
      "97280\n",
      "97408\n",
      "97536\n",
      "97664\n",
      "97792\n",
      "97920\n",
      "98048\n",
      "98176\n",
      "98304\n",
      "98432\n",
      "98560\n",
      "98688\n",
      "98816\n",
      "98944\n",
      "99072\n",
      "99200\n",
      "99328\n",
      "99456\n",
      "99584\n",
      "99712\n",
      "99840\n",
      "99968\n",
      "100096\n",
      "100224\n",
      "100352\n",
      "100480\n",
      "100608\n",
      "100736\n",
      "100864\n",
      "100992\n",
      "101120\n",
      "101248\n",
      "101376\n",
      "101504\n",
      "101632\n",
      "101760\n",
      "101888\n",
      "102016\n",
      "102144\n",
      "102272\n",
      "102400\n",
      "102528\n",
      "102656\n",
      "102784\n",
      "102912\n",
      "103040\n",
      "103168\n",
      "103296\n",
      "103424\n",
      "103552\n",
      "103680\n",
      "103808\n",
      "103936\n",
      "104064\n",
      "104192\n",
      "104320\n",
      "104448\n",
      "104576\n",
      "104704\n",
      "104832\n",
      "104960\n",
      "105088\n",
      "105216\n",
      "105344\n",
      "105472\n",
      "105600\n",
      "105728\n",
      "105856\n",
      "105984\n",
      "106112\n",
      "106240\n",
      "106368\n",
      "106496\n",
      "106624\n",
      "106752\n",
      "106880\n",
      "107008\n",
      "107136\n",
      "107264\n",
      "107392\n",
      "107520\n",
      "107648\n",
      "107776\n",
      "107904\n",
      "108032\n",
      "108160\n",
      "108288\n",
      "108416\n",
      "108544\n",
      "108672\n",
      "108800\n",
      "108928\n",
      "109056\n",
      "109184\n",
      "109312\n",
      "109440\n",
      "109568\n",
      "109696\n",
      "109824\n",
      "109952\n",
      "110080\n",
      "110208\n",
      "110336\n",
      "110464\n",
      "110592\n",
      "110720\n",
      "110848\n",
      "110976\n",
      "111104\n",
      "111232\n",
      "111360\n",
      "111488\n",
      "111616\n",
      "111744\n",
      "111872\n",
      "112000\n",
      "112128\n",
      "112256\n",
      "112384\n",
      "112512\n",
      "112640\n",
      "112768\n",
      "112896\n",
      "113024\n",
      "113152\n",
      "113280\n",
      "113408\n",
      "113536\n",
      "113664\n",
      "113792\n",
      "113920\n",
      "114048\n",
      "114176\n",
      "114304\n",
      "114432\n",
      "114560\n",
      "114688\n",
      "114816\n",
      "114944\n",
      "115072\n",
      "115200\n",
      "115328\n",
      "115456\n",
      "115584\n",
      "115712\n",
      "115840\n",
      "115968\n",
      "116096\n",
      "116224\n",
      "116352\n",
      "116480\n",
      "116608\n",
      "116736\n",
      "116864\n",
      "116992\n",
      "117120\n",
      "117248\n",
      "117376\n",
      "117504\n",
      "117632\n",
      "117760\n",
      "117888\n",
      "118016\n",
      "118144\n",
      "118272\n",
      "118400\n",
      "118528\n",
      "118656\n",
      "118784\n",
      "118912\n",
      "119040\n",
      "119168\n",
      "119296\n",
      "119424\n",
      "119552\n",
      "119680\n",
      "119808\n",
      "119936\n",
      "120064\n",
      "120192\n",
      "120320\n",
      "120448\n",
      "120576\n",
      "120704\n",
      "120832\n",
      "120960\n",
      "121088\n",
      "121216\n",
      "121344\n",
      "121472\n",
      "121600\n",
      "121728\n",
      "121856\n",
      "121984\n",
      "122112\n",
      "122240\n",
      "122368\n",
      "122496\n",
      "122624\n",
      "122752\n",
      "122880\n",
      "123008\n",
      "123136\n",
      "123264\n",
      "123392\n",
      "123520\n",
      "123648\n",
      "123776\n",
      "123904\n",
      "124032\n",
      "124160\n",
      "124288\n",
      "124416\n",
      "124544\n",
      "124672\n",
      "124800\n",
      "124928\n",
      "125056\n",
      "125184\n",
      "125312\n",
      "125440\n",
      "125568\n",
      "125696\n",
      "125824\n",
      "125952\n",
      "126080\n",
      "126208\n",
      "126336\n",
      "126464\n",
      "126592\n",
      "126720\n",
      "126848\n",
      "126976\n",
      "127104\n",
      "127232\n",
      "127360\n",
      "127488\n",
      "127616\n",
      "127744\n",
      "127872\n",
      "128000\n",
      "Minibatch loss at step 1000: 0.903325\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 77.8%\n",
      "128128\n",
      "128256\n",
      "128384\n",
      "128512\n",
      "128640\n",
      "128768\n",
      "128896\n",
      "129024\n",
      "129152\n",
      "129280\n",
      "129408\n",
      "129536\n",
      "129664\n",
      "129792\n",
      "129920\n",
      "130048\n",
      "130176\n",
      "130304\n",
      "130432\n",
      "130560\n",
      "130688\n",
      "130816\n",
      "130944\n",
      "131072\n",
      "131200\n",
      "131328\n",
      "131456\n",
      "131584\n",
      "131712\n",
      "131840\n",
      "131968\n",
      "132096\n",
      "132224\n",
      "132352\n",
      "132480\n",
      "132608\n",
      "132736\n",
      "132864\n",
      "132992\n",
      "133120\n",
      "133248\n",
      "133376\n",
      "133504\n",
      "133632\n",
      "133760\n",
      "133888\n",
      "134016\n",
      "134144\n",
      "134272\n",
      "134400\n",
      "134528\n",
      "134656\n",
      "134784\n",
      "134912\n",
      "135040\n",
      "135168\n",
      "135296\n",
      "135424\n",
      "135552\n",
      "135680\n",
      "135808\n",
      "135936\n",
      "136064\n",
      "136192\n",
      "136320\n",
      "136448\n",
      "136576\n",
      "136704\n",
      "136832\n",
      "136960\n",
      "137088\n",
      "137216\n",
      "137344\n",
      "137472\n",
      "137600\n",
      "137728\n",
      "137856\n",
      "137984\n",
      "138112\n",
      "138240\n",
      "138368\n",
      "138496\n",
      "138624\n",
      "138752\n",
      "138880\n",
      "139008\n",
      "139136\n",
      "139264\n",
      "139392\n",
      "139520\n",
      "139648\n",
      "139776\n",
      "139904\n",
      "140032\n",
      "140160\n",
      "140288\n",
      "140416\n",
      "140544\n",
      "140672\n",
      "140800\n",
      "140928\n",
      "141056\n",
      "141184\n",
      "141312\n",
      "141440\n",
      "141568\n",
      "141696\n",
      "141824\n",
      "141952\n",
      "142080\n",
      "142208\n",
      "142336\n",
      "142464\n",
      "142592\n",
      "142720\n",
      "142848\n",
      "142976\n",
      "143104\n",
      "143232\n",
      "143360\n",
      "143488\n",
      "143616\n",
      "143744\n",
      "143872\n",
      "144000\n",
      "144128\n",
      "144256\n",
      "144384\n",
      "144512\n",
      "144640\n",
      "144768\n",
      "144896\n",
      "145024\n",
      "145152\n",
      "145280\n",
      "145408\n",
      "145536\n",
      "145664\n",
      "145792\n",
      "145920\n",
      "146048\n",
      "146176\n",
      "146304\n",
      "146432\n",
      "146560\n",
      "146688\n",
      "146816\n",
      "146944\n",
      "147072\n",
      "147200\n",
      "147328\n",
      "147456\n",
      "147584\n",
      "147712\n",
      "147840\n",
      "147968\n",
      "148096\n",
      "148224\n",
      "148352\n",
      "148480\n",
      "148608\n",
      "148736\n",
      "148864\n",
      "148992\n",
      "149120\n",
      "149248\n",
      "149376\n",
      "149504\n",
      "149632\n",
      "149760\n",
      "149888\n",
      "150016\n",
      "150144\n",
      "150272\n",
      "150400\n",
      "150528\n",
      "150656\n",
      "150784\n",
      "150912\n",
      "151040\n",
      "151168\n",
      "151296\n",
      "151424\n",
      "151552\n",
      "151680\n",
      "151808\n",
      "151936\n",
      "152064\n",
      "152192\n",
      "152320\n",
      "152448\n",
      "152576\n",
      "152704\n",
      "152832\n",
      "152960\n",
      "153088\n",
      "153216\n",
      "153344\n",
      "153472\n",
      "153600\n",
      "153728\n",
      "153856\n",
      "153984\n",
      "154112\n",
      "154240\n",
      "154368\n",
      "154496\n",
      "154624\n",
      "154752\n",
      "154880\n",
      "155008\n",
      "155136\n",
      "155264\n",
      "155392\n",
      "155520\n",
      "155648\n",
      "155776\n",
      "155904\n",
      "156032\n",
      "156160\n",
      "156288\n",
      "156416\n",
      "156544\n",
      "156672\n",
      "156800\n",
      "156928\n",
      "157056\n",
      "157184\n",
      "157312\n",
      "157440\n",
      "157568\n",
      "157696\n",
      "157824\n",
      "157952\n",
      "158080\n",
      "158208\n",
      "158336\n",
      "158464\n",
      "158592\n",
      "158720\n",
      "158848\n",
      "158976\n",
      "159104\n",
      "159232\n",
      "159360\n",
      "159488\n",
      "159616\n",
      "159744\n",
      "159872\n",
      "160000\n",
      "160128\n",
      "160256\n",
      "160384\n",
      "160512\n",
      "160640\n",
      "160768\n",
      "160896\n",
      "161024\n",
      "161152\n",
      "161280\n",
      "161408\n",
      "161536\n",
      "161664\n",
      "161792\n",
      "161920\n",
      "162048\n",
      "162176\n",
      "162304\n",
      "162432\n",
      "162560\n",
      "162688\n",
      "162816\n",
      "162944\n",
      "163072\n",
      "163200\n",
      "163328\n",
      "163456\n",
      "163584\n",
      "163712\n",
      "163840\n",
      "163968\n",
      "164096\n",
      "164224\n",
      "164352\n",
      "164480\n",
      "164608\n",
      "164736\n",
      "164864\n",
      "164992\n",
      "165120\n",
      "165248\n",
      "165376\n",
      "165504\n",
      "165632\n",
      "165760\n",
      "165888\n",
      "166016\n",
      "166144\n",
      "166272\n",
      "166400\n",
      "166528\n",
      "166656\n",
      "166784\n",
      "166912\n",
      "167040\n",
      "167168\n",
      "167296\n",
      "167424\n",
      "167552\n",
      "167680\n",
      "167808\n",
      "167936\n",
      "168064\n",
      "168192\n",
      "168320\n",
      "168448\n",
      "168576\n",
      "168704\n",
      "168832\n",
      "168960\n",
      "169088\n",
      "169216\n",
      "169344\n",
      "169472\n",
      "169600\n",
      "169728\n",
      "169856\n",
      "169984\n",
      "170112\n",
      "170240\n",
      "170368\n",
      "170496\n",
      "170624\n",
      "170752\n",
      "170880\n",
      "171008\n",
      "171136\n",
      "171264\n",
      "171392\n",
      "171520\n",
      "171648\n",
      "171776\n",
      "171904\n",
      "172032\n",
      "172160\n",
      "172288\n",
      "172416\n",
      "172544\n",
      "172672\n",
      "172800\n",
      "172928\n",
      "173056\n",
      "173184\n",
      "173312\n",
      "173440\n",
      "173568\n",
      "173696\n",
      "173824\n",
      "173952\n",
      "174080\n",
      "174208\n",
      "174336\n",
      "174464\n",
      "174592\n",
      "174720\n",
      "174848\n",
      "174976\n",
      "175104\n",
      "175232\n",
      "175360\n",
      "175488\n",
      "175616\n",
      "175744\n",
      "175872\n",
      "176000\n",
      "176128\n",
      "176256\n",
      "176384\n",
      "176512\n",
      "176640\n",
      "176768\n",
      "176896\n",
      "177024\n",
      "177152\n",
      "177280\n",
      "177408\n",
      "177536\n",
      "177664\n",
      "177792\n",
      "177920\n",
      "178048\n",
      "178176\n",
      "178304\n",
      "178432\n",
      "178560\n",
      "178688\n",
      "178816\n",
      "178944\n",
      "179072\n",
      "179200\n",
      "179328\n",
      "179456\n",
      "179584\n",
      "179712\n",
      "179840\n",
      "179968\n",
      "180096\n",
      "180224\n",
      "180352\n",
      "180480\n",
      "180608\n",
      "180736\n",
      "180864\n",
      "180992\n",
      "181120\n",
      "181248\n",
      "181376\n",
      "181504\n",
      "181632\n",
      "181760\n",
      "181888\n",
      "182016\n",
      "182144\n",
      "182272\n",
      "182400\n",
      "182528\n",
      "182656\n",
      "182784\n",
      "182912\n",
      "183040\n",
      "183168\n",
      "183296\n",
      "183424\n",
      "183552\n",
      "183680\n",
      "183808\n",
      "183936\n",
      "184064\n",
      "184192\n",
      "184320\n",
      "184448\n",
      "184576\n",
      "184704\n",
      "184832\n",
      "184960\n",
      "185088\n",
      "185216\n",
      "185344\n",
      "185472\n",
      "185600\n",
      "185728\n",
      "185856\n",
      "185984\n",
      "186112\n",
      "186240\n",
      "186368\n",
      "186496\n",
      "186624\n",
      "186752\n",
      "186880\n",
      "187008\n",
      "187136\n",
      "187264\n",
      "187392\n",
      "187520\n",
      "187648\n",
      "187776\n",
      "187904\n",
      "188032\n",
      "188160\n",
      "188288\n",
      "188416\n",
      "188544\n",
      "188672\n",
      "188800\n",
      "188928\n",
      "189056\n",
      "189184\n",
      "189312\n",
      "189440\n",
      "189568\n",
      "189696\n",
      "189824\n",
      "189952\n",
      "190080\n",
      "190208\n",
      "190336\n",
      "190464\n",
      "190592\n",
      "190720\n",
      "190848\n",
      "190976\n",
      "191104\n",
      "191232\n",
      "191360\n",
      "191488\n",
      "191616\n",
      "191744\n",
      "191872\n",
      "192000\n",
      "Minibatch loss at step 1500: 1.431309\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 78.2%\n",
      "192128\n",
      "192256\n",
      "192384\n",
      "192512\n",
      "192640\n",
      "192768\n",
      "192896\n",
      "193024\n",
      "193152\n",
      "193280\n",
      "193408\n",
      "193536\n",
      "193664\n",
      "193792\n",
      "193920\n",
      "194048\n",
      "194176\n",
      "194304\n",
      "194432\n",
      "194560\n",
      "194688\n",
      "194816\n",
      "194944\n",
      "195072\n",
      "195200\n",
      "195328\n",
      "195456\n",
      "195584\n",
      "195712\n",
      "195840\n",
      "195968\n",
      "196096\n",
      "196224\n",
      "196352\n",
      "196480\n",
      "196608\n",
      "196736\n",
      "196864\n",
      "196992\n",
      "197120\n",
      "197248\n",
      "197376\n",
      "197504\n",
      "197632\n",
      "197760\n",
      "197888\n",
      "198016\n",
      "198144\n",
      "198272\n",
      "198400\n",
      "198528\n",
      "198656\n",
      "198784\n",
      "198912\n",
      "199040\n",
      "199168\n",
      "199296\n",
      "199424\n",
      "199552\n",
      "199680\n",
      "199808\n",
      "64\n",
      "192\n",
      "320\n",
      "448\n",
      "576\n",
      "704\n",
      "832\n",
      "960\n",
      "1088\n",
      "1216\n",
      "1344\n",
      "1472\n",
      "1600\n",
      "1728\n",
      "1856\n",
      "1984\n",
      "2112\n",
      "2240\n",
      "2368\n",
      "2496\n",
      "2624\n",
      "2752\n",
      "2880\n",
      "3008\n",
      "3136\n",
      "3264\n",
      "3392\n",
      "3520\n",
      "3648\n",
      "3776\n",
      "3904\n",
      "4032\n",
      "4160\n",
      "4288\n",
      "4416\n",
      "4544\n",
      "4672\n",
      "4800\n",
      "4928\n",
      "5056\n",
      "5184\n",
      "5312\n",
      "5440\n",
      "5568\n",
      "5696\n",
      "5824\n",
      "5952\n",
      "6080\n",
      "6208\n",
      "6336\n",
      "6464\n",
      "6592\n",
      "6720\n",
      "6848\n",
      "6976\n",
      "7104\n",
      "7232\n",
      "7360\n",
      "7488\n",
      "7616\n",
      "7744\n",
      "7872\n",
      "8000\n",
      "8128\n",
      "8256\n",
      "8384\n",
      "8512\n",
      "8640\n",
      "8768\n",
      "8896\n",
      "9024\n",
      "9152\n",
      "9280\n",
      "9408\n",
      "9536\n",
      "9664\n",
      "9792\n",
      "9920\n",
      "10048\n",
      "10176\n",
      "10304\n",
      "10432\n",
      "10560\n",
      "10688\n",
      "10816\n",
      "10944\n",
      "11072\n",
      "11200\n",
      "11328\n",
      "11456\n",
      "11584\n",
      "11712\n",
      "11840\n",
      "11968\n",
      "12096\n",
      "12224\n",
      "12352\n",
      "12480\n",
      "12608\n",
      "12736\n",
      "12864\n",
      "12992\n",
      "13120\n",
      "13248\n",
      "13376\n",
      "13504\n",
      "13632\n",
      "13760\n",
      "13888\n",
      "14016\n",
      "14144\n",
      "14272\n",
      "14400\n",
      "14528\n",
      "14656\n",
      "14784\n",
      "14912\n",
      "15040\n",
      "15168\n",
      "15296\n",
      "15424\n",
      "15552\n",
      "15680\n",
      "15808\n",
      "15936\n",
      "16064\n",
      "16192\n",
      "16320\n",
      "16448\n",
      "16576\n",
      "16704\n",
      "16832\n",
      "16960\n",
      "17088\n",
      "17216\n",
      "17344\n",
      "17472\n",
      "17600\n",
      "17728\n",
      "17856\n",
      "17984\n",
      "18112\n",
      "18240\n",
      "18368\n",
      "18496\n",
      "18624\n",
      "18752\n",
      "18880\n",
      "19008\n",
      "19136\n",
      "19264\n",
      "19392\n",
      "19520\n",
      "19648\n",
      "19776\n",
      "19904\n",
      "20032\n",
      "20160\n",
      "20288\n",
      "20416\n",
      "20544\n",
      "20672\n",
      "20800\n",
      "20928\n",
      "21056\n",
      "21184\n",
      "21312\n",
      "21440\n",
      "21568\n",
      "21696\n",
      "21824\n",
      "21952\n",
      "22080\n",
      "22208\n",
      "22336\n",
      "22464\n",
      "22592\n",
      "22720\n",
      "22848\n",
      "22976\n",
      "23104\n",
      "23232\n",
      "23360\n",
      "23488\n",
      "23616\n",
      "23744\n",
      "23872\n",
      "24000\n",
      "24128\n",
      "24256\n",
      "24384\n",
      "24512\n",
      "24640\n",
      "24768\n",
      "24896\n",
      "25024\n",
      "25152\n",
      "25280\n",
      "25408\n",
      "25536\n",
      "25664\n",
      "25792\n",
      "25920\n",
      "26048\n",
      "26176\n",
      "26304\n",
      "26432\n",
      "26560\n",
      "26688\n",
      "26816\n",
      "26944\n",
      "27072\n",
      "27200\n",
      "27328\n",
      "27456\n",
      "27584\n",
      "27712\n",
      "27840\n",
      "27968\n",
      "28096\n",
      "28224\n",
      "28352\n",
      "28480\n",
      "28608\n",
      "28736\n",
      "28864\n",
      "28992\n",
      "29120\n",
      "29248\n",
      "29376\n",
      "29504\n",
      "29632\n",
      "29760\n",
      "29888\n",
      "30016\n",
      "30144\n",
      "30272\n",
      "30400\n",
      "30528\n",
      "30656\n",
      "30784\n",
      "30912\n",
      "31040\n",
      "31168\n",
      "31296\n",
      "31424\n",
      "31552\n",
      "31680\n",
      "31808\n",
      "31936\n",
      "32064\n",
      "32192\n",
      "32320\n",
      "32448\n",
      "32576\n",
      "32704\n",
      "32832\n",
      "32960\n",
      "33088\n",
      "33216\n",
      "33344\n",
      "33472\n",
      "33600\n",
      "33728\n",
      "33856\n",
      "33984\n",
      "34112\n",
      "34240\n",
      "34368\n",
      "34496\n",
      "34624\n",
      "34752\n",
      "34880\n",
      "35008\n",
      "35136\n",
      "35264\n",
      "35392\n",
      "35520\n",
      "35648\n",
      "35776\n",
      "35904\n",
      "36032\n",
      "36160\n",
      "36288\n",
      "36416\n",
      "36544\n",
      "36672\n",
      "36800\n",
      "36928\n",
      "37056\n",
      "37184\n",
      "37312\n",
      "37440\n",
      "37568\n",
      "37696\n",
      "37824\n",
      "37952\n",
      "38080\n",
      "38208\n",
      "38336\n",
      "38464\n",
      "38592\n",
      "38720\n",
      "38848\n",
      "38976\n",
      "39104\n",
      "39232\n",
      "39360\n",
      "39488\n",
      "39616\n",
      "39744\n",
      "39872\n",
      "40000\n",
      "40128\n",
      "40256\n",
      "40384\n",
      "40512\n",
      "40640\n",
      "40768\n",
      "40896\n",
      "41024\n",
      "41152\n",
      "41280\n",
      "41408\n",
      "41536\n",
      "41664\n",
      "41792\n",
      "41920\n",
      "42048\n",
      "42176\n",
      "42304\n",
      "42432\n",
      "42560\n",
      "42688\n",
      "42816\n",
      "42944\n",
      "43072\n",
      "43200\n",
      "43328\n",
      "43456\n",
      "43584\n",
      "43712\n",
      "43840\n",
      "43968\n",
      "44096\n",
      "44224\n",
      "44352\n",
      "44480\n",
      "44608\n",
      "44736\n",
      "44864\n",
      "44992\n",
      "45120\n",
      "45248\n",
      "45376\n",
      "45504\n",
      "45632\n",
      "45760\n",
      "45888\n",
      "46016\n",
      "46144\n",
      "46272\n",
      "46400\n",
      "46528\n",
      "46656\n",
      "46784\n",
      "46912\n",
      "47040\n",
      "47168\n",
      "47296\n",
      "47424\n",
      "47552\n",
      "47680\n",
      "47808\n",
      "47936\n",
      "48064\n",
      "48192\n",
      "48320\n",
      "48448\n",
      "48576\n",
      "48704\n",
      "48832\n",
      "48960\n",
      "49088\n",
      "49216\n",
      "49344\n",
      "49472\n",
      "49600\n",
      "49728\n",
      "49856\n",
      "49984\n",
      "50112\n",
      "50240\n",
      "50368\n",
      "50496\n",
      "50624\n",
      "50752\n",
      "50880\n",
      "51008\n",
      "51136\n",
      "51264\n",
      "51392\n",
      "51520\n",
      "51648\n",
      "51776\n",
      "51904\n",
      "52032\n",
      "52160\n",
      "52288\n",
      "52416\n",
      "52544\n",
      "52672\n",
      "52800\n",
      "52928\n",
      "53056\n",
      "53184\n",
      "53312\n",
      "53440\n",
      "53568\n",
      "53696\n",
      "53824\n",
      "53952\n",
      "54080\n",
      "54208\n",
      "54336\n",
      "54464\n",
      "54592\n",
      "54720\n",
      "54848\n",
      "54976\n",
      "55104\n",
      "55232\n",
      "55360\n",
      "55488\n",
      "55616\n",
      "55744\n",
      "55872\n",
      "56000\n",
      "56128\n",
      "Minibatch loss at step 2000: 1.113893\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 78.5%\n",
      "56256\n",
      "56384\n",
      "56512\n",
      "56640\n",
      "56768\n",
      "56896\n",
      "57024\n",
      "57152\n",
      "57280\n",
      "57408\n",
      "57536\n",
      "57664\n",
      "57792\n",
      "57920\n",
      "58048\n",
      "58176\n",
      "58304\n",
      "58432\n",
      "58560\n",
      "58688\n",
      "58816\n",
      "58944\n",
      "59072\n",
      "59200\n",
      "59328\n",
      "59456\n",
      "59584\n",
      "59712\n",
      "59840\n",
      "59968\n",
      "60096\n",
      "60224\n",
      "60352\n",
      "60480\n",
      "60608\n",
      "60736\n",
      "60864\n",
      "60992\n",
      "61120\n",
      "61248\n",
      "61376\n",
      "61504\n",
      "61632\n",
      "61760\n",
      "61888\n",
      "62016\n",
      "62144\n",
      "62272\n",
      "62400\n",
      "62528\n",
      "62656\n",
      "62784\n",
      "62912\n",
      "63040\n",
      "63168\n",
      "63296\n",
      "63424\n",
      "63552\n",
      "63680\n",
      "63808\n",
      "63936\n",
      "64064\n",
      "64192\n",
      "64320\n",
      "64448\n",
      "64576\n",
      "64704\n",
      "64832\n",
      "64960\n",
      "65088\n",
      "65216\n",
      "65344\n",
      "65472\n",
      "65600\n",
      "65728\n",
      "65856\n",
      "65984\n",
      "66112\n",
      "66240\n",
      "66368\n",
      "66496\n",
      "66624\n",
      "66752\n",
      "66880\n",
      "67008\n",
      "67136\n",
      "67264\n",
      "67392\n",
      "67520\n",
      "67648\n",
      "67776\n",
      "67904\n",
      "68032\n",
      "68160\n",
      "68288\n",
      "68416\n",
      "68544\n",
      "68672\n",
      "68800\n",
      "68928\n",
      "69056\n",
      "69184\n",
      "69312\n",
      "69440\n",
      "69568\n",
      "69696\n",
      "69824\n",
      "69952\n",
      "70080\n",
      "70208\n",
      "70336\n",
      "70464\n",
      "70592\n",
      "70720\n",
      "70848\n",
      "70976\n",
      "71104\n",
      "71232\n",
      "71360\n",
      "71488\n",
      "71616\n",
      "71744\n",
      "71872\n",
      "72000\n",
      "72128\n",
      "72256\n",
      "72384\n",
      "72512\n",
      "72640\n",
      "72768\n",
      "72896\n",
      "73024\n",
      "73152\n",
      "73280\n",
      "73408\n",
      "73536\n",
      "73664\n",
      "73792\n",
      "73920\n",
      "74048\n",
      "74176\n",
      "74304\n",
      "74432\n",
      "74560\n",
      "74688\n",
      "74816\n",
      "74944\n",
      "75072\n",
      "75200\n",
      "75328\n",
      "75456\n",
      "75584\n",
      "75712\n",
      "75840\n",
      "75968\n",
      "76096\n",
      "76224\n",
      "76352\n",
      "76480\n",
      "76608\n",
      "76736\n",
      "76864\n",
      "76992\n",
      "77120\n",
      "77248\n",
      "77376\n",
      "77504\n",
      "77632\n",
      "77760\n",
      "77888\n",
      "78016\n",
      "78144\n",
      "78272\n",
      "78400\n",
      "78528\n",
      "78656\n",
      "78784\n",
      "78912\n",
      "79040\n",
      "79168\n",
      "79296\n",
      "79424\n",
      "79552\n",
      "79680\n",
      "79808\n",
      "79936\n",
      "80064\n",
      "80192\n",
      "80320\n",
      "80448\n",
      "80576\n",
      "80704\n",
      "80832\n",
      "80960\n",
      "81088\n",
      "81216\n",
      "81344\n",
      "81472\n",
      "81600\n",
      "81728\n",
      "81856\n",
      "81984\n",
      "82112\n",
      "82240\n",
      "82368\n",
      "82496\n",
      "82624\n",
      "82752\n",
      "82880\n",
      "83008\n",
      "83136\n",
      "83264\n",
      "83392\n",
      "83520\n",
      "83648\n",
      "83776\n",
      "83904\n",
      "84032\n",
      "84160\n",
      "84288\n",
      "84416\n",
      "84544\n",
      "84672\n",
      "84800\n",
      "84928\n",
      "85056\n",
      "85184\n",
      "85312\n",
      "85440\n",
      "85568\n",
      "85696\n",
      "85824\n",
      "85952\n",
      "86080\n",
      "86208\n",
      "86336\n",
      "86464\n",
      "86592\n",
      "86720\n",
      "86848\n",
      "86976\n",
      "87104\n",
      "87232\n",
      "87360\n",
      "87488\n",
      "87616\n",
      "87744\n",
      "87872\n",
      "88000\n",
      "88128\n",
      "88256\n",
      "88384\n",
      "88512\n",
      "88640\n",
      "88768\n",
      "88896\n",
      "89024\n",
      "89152\n",
      "89280\n",
      "89408\n",
      "89536\n",
      "89664\n",
      "89792\n",
      "89920\n",
      "90048\n",
      "90176\n",
      "90304\n",
      "90432\n",
      "90560\n",
      "90688\n",
      "90816\n",
      "90944\n",
      "91072\n",
      "91200\n",
      "91328\n",
      "91456\n",
      "91584\n",
      "91712\n",
      "91840\n",
      "91968\n",
      "92096\n",
      "92224\n",
      "92352\n",
      "92480\n",
      "92608\n",
      "92736\n",
      "92864\n",
      "92992\n",
      "93120\n",
      "93248\n",
      "93376\n",
      "93504\n",
      "93632\n",
      "93760\n",
      "93888\n",
      "94016\n",
      "94144\n",
      "94272\n",
      "94400\n",
      "94528\n",
      "94656\n",
      "94784\n",
      "94912\n",
      "95040\n",
      "95168\n",
      "95296\n",
      "95424\n",
      "95552\n",
      "95680\n",
      "95808\n",
      "95936\n",
      "96064\n",
      "96192\n",
      "96320\n",
      "96448\n",
      "96576\n",
      "96704\n",
      "96832\n",
      "96960\n",
      "97088\n",
      "97216\n",
      "97344\n",
      "97472\n",
      "97600\n",
      "97728\n",
      "97856\n",
      "97984\n",
      "98112\n",
      "98240\n",
      "98368\n",
      "98496\n",
      "98624\n",
      "98752\n",
      "98880\n",
      "99008\n",
      "99136\n",
      "99264\n",
      "99392\n",
      "99520\n",
      "99648\n",
      "99776\n",
      "99904\n",
      "100032\n",
      "100160\n",
      "100288\n",
      "100416\n",
      "100544\n",
      "100672\n",
      "100800\n",
      "100928\n",
      "101056\n",
      "101184\n",
      "101312\n",
      "101440\n",
      "101568\n",
      "101696\n",
      "101824\n",
      "101952\n",
      "102080\n",
      "102208\n",
      "102336\n",
      "102464\n",
      "102592\n",
      "102720\n",
      "102848\n",
      "102976\n",
      "103104\n",
      "103232\n",
      "103360\n",
      "103488\n",
      "103616\n",
      "103744\n",
      "103872\n",
      "104000\n",
      "104128\n",
      "104256\n",
      "104384\n",
      "104512\n",
      "104640\n",
      "104768\n",
      "104896\n",
      "105024\n",
      "105152\n",
      "105280\n",
      "105408\n",
      "105536\n",
      "105664\n",
      "105792\n",
      "105920\n",
      "106048\n",
      "106176\n",
      "106304\n",
      "106432\n",
      "106560\n",
      "106688\n",
      "106816\n",
      "106944\n",
      "107072\n",
      "107200\n",
      "107328\n",
      "107456\n",
      "107584\n",
      "107712\n",
      "107840\n",
      "107968\n",
      "108096\n",
      "108224\n",
      "108352\n",
      "108480\n",
      "108608\n",
      "108736\n",
      "108864\n",
      "108992\n",
      "109120\n",
      "109248\n",
      "109376\n",
      "109504\n",
      "109632\n",
      "109760\n",
      "109888\n",
      "110016\n",
      "110144\n",
      "110272\n",
      "110400\n",
      "110528\n",
      "110656\n",
      "110784\n",
      "110912\n",
      "111040\n",
      "111168\n",
      "111296\n",
      "111424\n",
      "111552\n",
      "111680\n",
      "111808\n",
      "111936\n",
      "112064\n",
      "112192\n",
      "112320\n",
      "112448\n",
      "112576\n",
      "112704\n",
      "112832\n",
      "112960\n",
      "113088\n",
      "113216\n",
      "113344\n",
      "113472\n",
      "113600\n",
      "113728\n",
      "113856\n",
      "113984\n",
      "114112\n",
      "114240\n",
      "114368\n",
      "114496\n",
      "114624\n",
      "114752\n",
      "114880\n",
      "115008\n",
      "115136\n",
      "115264\n",
      "115392\n",
      "115520\n",
      "115648\n",
      "115776\n",
      "115904\n",
      "116032\n",
      "116160\n",
      "116288\n",
      "116416\n",
      "116544\n",
      "116672\n",
      "116800\n",
      "116928\n",
      "117056\n",
      "117184\n",
      "117312\n",
      "117440\n",
      "117568\n",
      "117696\n",
      "117824\n",
      "117952\n",
      "118080\n",
      "118208\n",
      "118336\n",
      "118464\n",
      "118592\n",
      "118720\n",
      "118848\n",
      "118976\n",
      "119104\n",
      "119232\n",
      "119360\n",
      "119488\n",
      "119616\n",
      "119744\n",
      "119872\n",
      "120000\n",
      "120128\n",
      "Minibatch loss at step 2500: 0.910151\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 78.8%\n",
      "120256\n",
      "120384\n",
      "120512\n",
      "120640\n",
      "120768\n",
      "120896\n",
      "121024\n",
      "121152\n",
      "121280\n",
      "121408\n",
      "121536\n",
      "121664\n",
      "121792\n",
      "121920\n",
      "122048\n",
      "122176\n",
      "122304\n",
      "122432\n",
      "122560\n",
      "122688\n",
      "122816\n",
      "122944\n",
      "123072\n",
      "123200\n",
      "123328\n",
      "123456\n",
      "123584\n",
      "123712\n",
      "123840\n",
      "123968\n",
      "124096\n",
      "124224\n",
      "124352\n",
      "124480\n",
      "124608\n",
      "124736\n",
      "124864\n",
      "124992\n",
      "125120\n",
      "125248\n",
      "125376\n",
      "125504\n",
      "125632\n",
      "125760\n",
      "125888\n",
      "126016\n",
      "126144\n",
      "126272\n",
      "126400\n",
      "126528\n",
      "126656\n",
      "126784\n",
      "126912\n",
      "127040\n",
      "127168\n",
      "127296\n",
      "127424\n",
      "127552\n",
      "127680\n",
      "127808\n",
      "127936\n",
      "128064\n",
      "128192\n",
      "128320\n",
      "128448\n",
      "128576\n",
      "128704\n",
      "128832\n",
      "128960\n",
      "129088\n",
      "129216\n",
      "129344\n",
      "129472\n",
      "129600\n",
      "129728\n",
      "129856\n",
      "129984\n",
      "130112\n",
      "130240\n",
      "130368\n",
      "130496\n",
      "130624\n",
      "130752\n",
      "130880\n",
      "131008\n",
      "131136\n",
      "131264\n",
      "131392\n",
      "131520\n",
      "131648\n",
      "131776\n",
      "131904\n",
      "132032\n",
      "132160\n",
      "132288\n",
      "132416\n",
      "132544\n",
      "132672\n",
      "132800\n",
      "132928\n",
      "133056\n",
      "133184\n",
      "133312\n",
      "133440\n",
      "133568\n",
      "133696\n",
      "133824\n",
      "133952\n",
      "134080\n",
      "134208\n",
      "134336\n",
      "134464\n",
      "134592\n",
      "134720\n",
      "134848\n",
      "134976\n",
      "135104\n",
      "135232\n",
      "135360\n",
      "135488\n",
      "135616\n",
      "135744\n",
      "135872\n",
      "136000\n",
      "136128\n",
      "136256\n",
      "136384\n",
      "136512\n",
      "136640\n",
      "136768\n",
      "136896\n",
      "137024\n",
      "137152\n",
      "137280\n",
      "137408\n",
      "137536\n",
      "137664\n",
      "137792\n",
      "137920\n",
      "138048\n",
      "138176\n",
      "138304\n",
      "138432\n",
      "138560\n",
      "138688\n",
      "138816\n",
      "138944\n",
      "139072\n",
      "139200\n",
      "139328\n",
      "139456\n",
      "139584\n",
      "139712\n",
      "139840\n",
      "139968\n",
      "140096\n",
      "140224\n",
      "140352\n",
      "140480\n",
      "140608\n",
      "140736\n",
      "140864\n",
      "140992\n",
      "141120\n",
      "141248\n",
      "141376\n",
      "141504\n",
      "141632\n",
      "141760\n",
      "141888\n",
      "142016\n",
      "142144\n",
      "142272\n",
      "142400\n",
      "142528\n",
      "142656\n",
      "142784\n",
      "142912\n",
      "143040\n",
      "143168\n",
      "143296\n",
      "143424\n",
      "143552\n",
      "143680\n",
      "143808\n",
      "143936\n",
      "144064\n",
      "144192\n",
      "144320\n",
      "144448\n",
      "144576\n",
      "144704\n",
      "144832\n",
      "144960\n",
      "145088\n",
      "145216\n",
      "145344\n",
      "145472\n",
      "145600\n",
      "145728\n",
      "145856\n",
      "145984\n",
      "146112\n",
      "146240\n",
      "146368\n",
      "146496\n",
      "146624\n",
      "146752\n",
      "146880\n",
      "147008\n",
      "147136\n",
      "147264\n",
      "147392\n",
      "147520\n",
      "147648\n",
      "147776\n",
      "147904\n",
      "148032\n",
      "148160\n",
      "148288\n",
      "148416\n",
      "148544\n",
      "148672\n",
      "148800\n",
      "148928\n",
      "149056\n",
      "149184\n",
      "149312\n",
      "149440\n",
      "149568\n",
      "149696\n",
      "149824\n",
      "149952\n",
      "150080\n",
      "150208\n",
      "150336\n",
      "150464\n",
      "150592\n",
      "150720\n",
      "150848\n",
      "150976\n",
      "151104\n",
      "151232\n",
      "151360\n",
      "151488\n",
      "151616\n",
      "151744\n",
      "151872\n",
      "152000\n",
      "152128\n",
      "152256\n",
      "152384\n",
      "152512\n",
      "152640\n",
      "152768\n",
      "152896\n",
      "153024\n",
      "153152\n",
      "153280\n",
      "153408\n",
      "153536\n",
      "153664\n",
      "153792\n",
      "153920\n",
      "154048\n",
      "154176\n",
      "154304\n",
      "154432\n",
      "154560\n",
      "154688\n",
      "154816\n",
      "154944\n",
      "155072\n",
      "155200\n",
      "155328\n",
      "155456\n",
      "155584\n",
      "155712\n",
      "155840\n",
      "155968\n",
      "156096\n",
      "156224\n",
      "156352\n",
      "156480\n",
      "156608\n",
      "156736\n",
      "156864\n",
      "156992\n",
      "157120\n",
      "157248\n",
      "157376\n",
      "157504\n",
      "157632\n",
      "157760\n",
      "157888\n",
      "158016\n",
      "158144\n",
      "158272\n",
      "158400\n",
      "158528\n",
      "158656\n",
      "158784\n",
      "158912\n",
      "159040\n",
      "159168\n",
      "159296\n",
      "159424\n",
      "159552\n",
      "159680\n",
      "159808\n",
      "159936\n",
      "160064\n",
      "160192\n",
      "160320\n",
      "160448\n",
      "160576\n",
      "160704\n",
      "160832\n",
      "160960\n",
      "161088\n",
      "161216\n",
      "161344\n",
      "161472\n",
      "161600\n",
      "161728\n",
      "161856\n",
      "161984\n",
      "162112\n",
      "162240\n",
      "162368\n",
      "162496\n",
      "162624\n",
      "162752\n",
      "162880\n",
      "163008\n",
      "163136\n",
      "163264\n",
      "163392\n",
      "163520\n",
      "163648\n",
      "163776\n",
      "163904\n",
      "164032\n",
      "164160\n",
      "164288\n",
      "164416\n",
      "164544\n",
      "164672\n",
      "164800\n",
      "164928\n",
      "165056\n",
      "165184\n",
      "165312\n",
      "165440\n",
      "165568\n",
      "165696\n",
      "165824\n",
      "165952\n",
      "166080\n",
      "166208\n",
      "166336\n",
      "166464\n",
      "166592\n",
      "166720\n",
      "166848\n",
      "166976\n",
      "167104\n",
      "167232\n",
      "167360\n",
      "167488\n",
      "167616\n",
      "167744\n",
      "167872\n",
      "168000\n",
      "168128\n",
      "168256\n",
      "168384\n",
      "168512\n",
      "168640\n",
      "168768\n",
      "168896\n",
      "169024\n",
      "169152\n",
      "169280\n",
      "169408\n",
      "169536\n",
      "169664\n",
      "169792\n",
      "169920\n",
      "170048\n",
      "170176\n",
      "170304\n",
      "170432\n",
      "170560\n",
      "170688\n",
      "170816\n",
      "170944\n",
      "171072\n",
      "171200\n",
      "171328\n",
      "171456\n",
      "171584\n",
      "171712\n",
      "171840\n",
      "171968\n",
      "172096\n",
      "172224\n",
      "172352\n",
      "172480\n",
      "172608\n",
      "172736\n",
      "172864\n",
      "172992\n",
      "173120\n",
      "173248\n",
      "173376\n",
      "173504\n",
      "173632\n",
      "173760\n",
      "173888\n",
      "174016\n",
      "174144\n",
      "174272\n",
      "174400\n",
      "174528\n",
      "174656\n",
      "174784\n",
      "174912\n",
      "175040\n",
      "175168\n",
      "175296\n",
      "175424\n",
      "175552\n",
      "175680\n",
      "175808\n",
      "175936\n",
      "176064\n",
      "176192\n",
      "176320\n",
      "176448\n",
      "176576\n",
      "176704\n",
      "176832\n",
      "176960\n",
      "177088\n",
      "177216\n",
      "177344\n",
      "177472\n",
      "177600\n",
      "177728\n",
      "177856\n",
      "177984\n",
      "178112\n",
      "178240\n",
      "178368\n",
      "178496\n",
      "178624\n",
      "178752\n",
      "178880\n",
      "179008\n",
      "179136\n",
      "179264\n",
      "179392\n",
      "179520\n",
      "179648\n",
      "179776\n",
      "179904\n",
      "180032\n",
      "180160\n",
      "180288\n",
      "180416\n",
      "180544\n",
      "180672\n",
      "180800\n",
      "180928\n",
      "181056\n",
      "181184\n",
      "181312\n",
      "181440\n",
      "181568\n",
      "181696\n",
      "181824\n",
      "181952\n",
      "182080\n",
      "182208\n",
      "182336\n",
      "182464\n",
      "182592\n",
      "182720\n",
      "182848\n",
      "182976\n",
      "183104\n",
      "183232\n",
      "183360\n",
      "183488\n",
      "183616\n",
      "183744\n",
      "183872\n",
      "184000\n",
      "184128\n",
      "Minibatch loss at step 3000: 0.826655\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 79.4%\n",
      "Test accuracy: 86.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001 # More steps (but faster!) since we're using batch gradient descent\n",
    "\n",
    "with tf.Session(graph=graph) as session: # Instantiate a session, passing the graph to it\n",
    "    tf.global_variables_initializer().run() # Initialize random variables\n",
    "    print(\"Initialized\")\n",
    "    for step in range(num_steps):\n",
    "        # Pick an offset within the training data, which has been randomized.\n",
    "        # Note: we could use better randomization across epochs.\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size) # To vary the minibatch taken\n",
    "        # Generate a minibatch.\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "        # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "        # and the value is the numpy array to feed to it.\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels} # We use a dictionary to pass values to tf.placeholder()s\n",
    "        _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict) # Step through an optimization iteration\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "            print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7omWxtvLLxik"
   },
   "source": [
    "---\n",
    "Problem\n",
    "-------\n",
    "\n",
    "Turn the logistic regression example with SGD into a 1-hidden layer neural network with rectified linear units [nn.relu()](https://www.tensorflow.org/versions/r0.7/api_docs/python/nn.html#relu) and 1024 hidden nodes. This model should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up the graph.\n",
    "# 1. Create placeholders for the minibatch training data and labels.\n",
    "# 2. Create constants for the test data (X) and labels.\n",
    "# 3. Create variables for the first-layer weights and biases. W is [input_D x 1024], b is [1024] (tf does the broadcasting)\n",
    "# 4. -> Pass XW + b to nn.relu()\n",
    "# 5. -> Relu will output an [n_datum x 1024] tensor, X\n",
    "# 6. -> Compute logits = XW + b, this time with W:[1024 x n_classes], b:[n_classes]\n",
    "# 7. -> Pass these values (the logits) to tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(train_labels, logits))\n",
    "# 8. -> Create an optimizer.\n",
    "# 8. -> Create prediction variables so that we can see how well the network is training.\n",
    "\n",
    "# Run it\n",
    "# 0. Specify a batch size.\n",
    "# 1. Start the session, pass the graph to it.\n",
    "# 2. Loop for n_steps,\n",
    "#    creating a new minibatch each time and passing it to the model via feed_dict{},\n",
    "#    calling [_, l, predictions] = session.run() afterwards each time to run an optimization step.\n",
    "# 3. Every kth iteration, print the loss, minibatch, and training error.\n",
    "# 4. On the final iteration, print the test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the graph\n",
    "batch_size = 256\n",
    "n_relus    = 2048\n",
    "\n",
    "graph2 = tf.Graph();\n",
    "with graph2.as_default():\n",
    "    # Data handles\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape = (batch_size, image_size * image_size))\n",
    "    tf_train_labels  = tf.placeholder(tf.float32, shape = (batch_size, num_labels)) # Labels are dummy encoded\n",
    "    tf_test_data     = tf.constant(test_dataset)\n",
    "    tf_valid_data    = tf.constant(valid_dataset)\n",
    "    \n",
    "    # Layer 1\n",
    "    weights_1        = tf.Variable(tf.truncated_normal([image_size * image_size, n_relus]))\n",
    "    biases_1         = tf.Variable(tf.zeros([n_relus]))\n",
    "    train_out_1      = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "    \n",
    "    # Layer 2\n",
    "    weights_2        = tf.Variable(tf.truncated_normal([n_relus, num_labels]))\n",
    "    biases_2         = tf.Variable(tf.truncated_normal([num_labels]))\n",
    "    train_out_2      = tf.matmul(train_out_1, weights_2) + biases_2\n",
    "    \n",
    "    # Loss calculation\n",
    "    loss             = tf.reduce_mean(\n",
    "                        tf.nn.softmax_cross_entropy_with_logits(labels = tf_train_labels, logits = train_out_2))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer        = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Training predictions\n",
    "    training_predictions = tf.nn.softmax(train_out_2, name = 'Abraham')\n",
    "    \n",
    "    # Validation predictions\n",
    "    valid_out_1          = tf.nn.relu(tf.matmul(tf_valid_data, weights_1) + biases_1)\n",
    "    valid_predictions    = tf.nn.softmax(tf.matmul(valid_out_1, weights_2) + biases_2, name = 'Peter')\n",
    "    \n",
    "    # Testing predictions\n",
    "    test_out_1           = tf.nn.relu(tf.matmul(tf_test_data, weights_1) + biases_1)\n",
    "    test_predictions     = tf.nn.softmax(tf.matmul(test_out_1, weights_2) + biases_2, name = 'Paul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 Performance\n",
      "-------------------\n",
      "Training set accuracy: 10.546875%\n",
      "Validation set accuracy: 36.29%\n",
      "Loss: 361.20697021484375\n",
      "\n",
      "Step 1000 Performance\n",
      "-------------------\n",
      "Training set accuracy: 80.46875%\n",
      "Validation set accuracy: 81.13%\n",
      "Loss: 14.469635009765625\n",
      "\n",
      "Step 2000 Performance\n",
      "-------------------\n",
      "Training set accuracy: 80.078125%\n",
      "Validation set accuracy: 81.22%\n",
      "Loss: 6.86448335647583\n",
      "\n",
      "Step 3000 Performance\n",
      "-------------------\n",
      "Training set accuracy: 82.8125%\n",
      "Validation set accuracy: 83.64%\n",
      "Loss: 2.46091628074646\n",
      "\n",
      "Step 4000 Performance\n",
      "-------------------\n",
      "Training set accuracy: 85.15625%\n",
      "Validation set accuracy: 84.18%\n",
      "Loss: 1.287778615951538\n",
      "\n",
      "Step 5000 Performance\n",
      "-------------------\n",
      "Training set accuracy: 85.15625%\n",
      "Validation set accuracy: 83.7%\n",
      "Loss: 1.5994062423706055\n",
      "\n",
      "Step 6000 Performance\n",
      "-------------------\n",
      "Training set accuracy: 84.765625%\n",
      "Validation set accuracy: 84.65%\n",
      "Loss: 2.489100694656372\n",
      "\n",
      "Step 7000 Performance\n",
      "-------------------\n",
      "Training set accuracy: 88.28125%\n",
      "Validation set accuracy: 84.68%\n",
      "Loss: 1.3192706108093262\n",
      "\n",
      "Step 8000 Performance\n",
      "-------------------\n",
      "Training set accuracy: 88.671875%\n",
      "Validation set accuracy: 84.87%\n",
      "Loss: 0.7396169900894165\n",
      "\n",
      "Step 9000 Performance\n",
      "-------------------\n",
      "Training set accuracy: 89.84375%\n",
      "Validation set accuracy: 84.75%\n",
      "Loss: 1.2066118717193604\n",
      "\n",
      "Step 10000 Performance\n",
      "-------------------\n",
      "Training set accuracy: 91.40625%\n",
      "Validation set accuracy: 85.15%\n",
      "Loss: 0.7559001445770264\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the session\n",
    "number_of_steps = 10001\n",
    "\n",
    "with tf.Session(graph = graph2) as session:\n",
    "    \n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    for step in range(number_of_steps):\n",
    "        # Create this step's minibatch:\n",
    "        batch_offset = (step * batch_size) % (train_dataset.shape[0] - batch_size)\n",
    "        batch_data   = train_dataset[batch_offset:batch_offset + batch_size, :]\n",
    "        batch_labels = train_labels[batch_offset:batch_offset + batch_size]\n",
    "        \n",
    "        # Pass that batch to the model via feed_dict\n",
    "        feed_dict            = {tf_train_dataset:batch_data, tf_train_labels:batch_labels}\n",
    "        _, l, predictions    = session.run([optimizer, loss, training_predictions], feed_dict = feed_dict)\n",
    "        \n",
    "        # Let us know how it's doing every 1000 steps\n",
    "        if (step % 1000 == 0):\n",
    "            print('Step {} Performance'.format(step))\n",
    "            print('-------------------')\n",
    "            print('Training set accuracy: {}%'.format(accuracy(predictions, batch_labels)))\n",
    "            print('Validation set accuracy: {}%'.format(accuracy(valid_predictions.eval(), valid_labels)))\n",
    "            print('Loss: {}'.format(l))\n",
    "            print('')\n",
    "            if step == number_of_steps:\n",
    "                print('Validation set accuracy: {}%'.format(accuracy(test_predictions.eval(), test_labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "2_fullyconnected.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
